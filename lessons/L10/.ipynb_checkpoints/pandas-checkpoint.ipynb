{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro to Pandas\n",
    "===============\n",
    "\n",
    "In the lessons covered in this course so far, we've covered \n",
    "the tools necessary to, given enough time and practice, perform \n",
    "most data analysis tasks that a scientist might want to do. However, \n",
    "like other things in Python, there are other tools that we \n",
    "can learn that can make that processes easier and more efficient. \n",
    "Pandas is data analysis toolbox created explicitly for this purpose.\n",
    "\n",
    "Arguably the most useful feature of Pandas is the addition of useful \n",
    "data analysis data types, specifically the `DataFrame`, which \n",
    "collects data in a table like format with rows and columns with the \n",
    "ability to index and describe the data with a single data frame \n",
    "itself. DataFrames \n",
    "are a relatively complex data type and it is beyond the scope of \n",
    "this course to go too\n",
    "far down the rabbit hole in learning how to use and manipulate them. \n",
    "The goal here is to introduce them to you and provide some basic working \n",
    "examples so that you can build on this knowledge in future courses and \n",
    "projects.\n",
    "\n",
    "On nice feature of Pandas is that the tools are built to play nicely with \n",
    "numpy. This means that we can do things like convert parts of a `DataFrame` \n",
    "to a numpy array very quickly. Additionally, Pandas provides tools \n",
    "that enable us to quickly write data in a variety of formats, including \n",
    "CSV, HTML, and a various flavors of SQL.\n",
    "\n",
    "Pandas Series\n",
    "-------------\n",
    "\n",
    "If you've been working with a recent version of Anaconda, then you should already have the \n",
    "ability to import pandas into you python session or programs. Before we \n",
    "get into working with the `DataFrame` type, let's first take a look at the Pandas `Series`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    20\n",
      "1    15\n",
      "2    18\n",
      "3    36\n",
      "4    35\n",
      "5    19\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mytemperatures = [20,15,18,36,35,19]\n",
    "myseries = pd.Series(mytemperatures)\n",
    "print(myseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right off, you should see there is a fundamental difference between the `Series` and \n",
    "the `Array`, as when I print the `Series`, there is some extra data that comes along with it. This is the \"index\" for each element in the series, and it is something that we can control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     20\n",
      "10    15\n",
      "20    18\n",
      "30    36\n",
      "40    35\n",
      "50    19\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mytimes = [0,10,20,30,40,50]\n",
    "myseries = pd.Series(mytemperatures,mytimes)\n",
    "print(myseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying the index of each element, what I am able to do is attach meaningful information (metadata) to my data. For example, here I've assigned a measurement time to each of my temperature measurements. In this way, I can do things like query my data using the index that I've assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(myseries.get(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead of a index number. Let me be a little more realistic for this example and make use of the python `date` object from the datetime module. A `date` object is just what it sounds like, a way to quickly and easily add dates to your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.date(2019, 1, 1), datetime.date(2019, 1, 3), datetime.date(2019, 1, 5), datetime.date(2019, 1, 7), datetime.date(2019, 1, 9), datetime.date(2019, 1, 11)]\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "year = 2019\n",
    "month = 1\n",
    "days = [1,3,5,7,9,11]\n",
    "mydates = []\n",
    "\n",
    "for d in days:\n",
    "    mydates.append(date(year,month,d))\n",
    "\n",
    "print(mydates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have a list of dates that I can use to index my temperature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01    20\n",
      "2019-01-03    15\n",
      "2019-01-05    18\n",
      "2019-01-07    36\n",
      "2019-01-09    35\n",
      "2019-01-11    19\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "myseries = pd.Series(mytemperatures,mydates)\n",
    "print(myseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if I want to grab the temperature for a specific day, I can use the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "adate = date(2019,1,7)\n",
    "atemperature = myseries.get(adate)\n",
    "print(atemperature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since the temperature series is indexed using a list of datetime `date` objects, then I need to use a `date` object when I \"get\" the data. In other words, I can't simply index with a string like \"2019-01-05\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "strdate = \"2019-01-05\"\n",
    "othertemperature = myseries.get(strdate)\n",
    "print(othertemperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas will allow you to query the `Series` but as you can see, the query returns a value \n",
    "of `None` since there is no index that matches such a string in my series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should think of the relationship between an index and the data in a `series` like you might \n",
    "think of a 2 column table in a spreadsheet program.\n",
    "\n",
    "![table1](images/table.png){width=\"300px\"}\n",
    "\n",
    "In this analogy, the number of rows in a \n",
    "spreadsheet corresponds to the number of elements in the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(myseries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas DataFrame\n",
    "----------------\n",
    "\n",
    "The Pandas `Series` is inherently a 1D data type with optional index labels. The `DataFrame` builds upon the idea of linking the data to the metadata to make it possible to store, query, and \n",
    "manipulate more complex 2D data. Let's extend our example from above and replicate this table\n",
    "\n",
    "![table2d](images/table2d.png){width=\"500px\"}\n",
    "\n",
    "using a `DataFrame`. I'll start this process by adding data in to a numpy array. Normally, this data would be read from a file as in previous examples/exercises in this class. Here I'll hard code it just to illustrate the point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20 16 18 30]\n",
      " [15 18 33 21]\n",
      " [18 28 19 13]\n",
      " [36 28 33 13]\n",
      " [35 28 28 17]\n",
      " [19 32 17 17]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "temperatureData = np.array([[20,16,18,30],\\\n",
    "                            [15,18,33,21],\\\n",
    "                            [18,28,19,13],\\\n",
    "                            [36,28,33,13],\\\n",
    "                            [35,28,28,17],\\\n",
    "                            [19,32,17,17]]\\\n",
    "                             )\n",
    "print(temperatureData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have all the data in an array. Note that I used the \"\\\" or line continuation character which allows me to write the array using multiple lines. Now, we can create a `DataFrame` and populate \n",
    "it with this data as well as the information about the dates from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Site A  Site B  Site C  Site D\n",
      "2019-01-01      20      16      18      30\n",
      "2019-01-03      15      18      33      21\n",
      "2019-01-05      18      28      19      13\n",
      "2019-01-07      36      28      33      13\n",
      "2019-01-09      35      28      28      17\n",
      "2019-01-11      19      32      17      17\n"
     ]
    }
   ],
   "source": [
    "columnNames = [\"Site A\",\"Site B\",\"Site C\",\"Site D\"]\n",
    "myDF = pd.DataFrame(temperatureData,index=mydates,columns=columnNames)\n",
    "print(myDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a proper table! \n",
    "\n",
    "As I said, typically we'd read data from a file, so let's see how to do that before moving on. As is often the case, I have some data in csv format. I've used python csv module in the past to read this data. Pandas also has a csv reader and it is even easier to use than what you've seen previously. \n",
    "\n",
    "I will be using the file [USC00200228.csv](USC00200228.csv) which contains rain, snowfall, snow depth, minimum, maximum, and observation temperature data each day for January - March 2020 in Ann Arbor, MI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"USC00200228.csv\"\n",
    "data = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, data files can have a lot of data, so I might not want to print the entire `DataFrame` here to examine it. A quick way to view a subset of the data is to use the `head()` and `tail()` methods, which \n",
    "print the beginning and end of the data frame respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       STATION                 NAME        DATE  PRCP  SNOW  SNWD  TMAX  TMIN  \\\n",
      "0  USC00200228  ANN ARBOR SE, MI US  2020-01-01  0.05   0.8   2.0    30    24   \n",
      "1  USC00200228  ANN ARBOR SE, MI US  2020-01-02  0.00   0.0   1.0    37    23   \n",
      "2  USC00200228  ANN ARBOR SE, MI US  2020-01-03  0.00   0.0   0.0    47    30   \n",
      "3  USC00200228  ANN ARBOR SE, MI US  2020-01-04  0.00   0.0   0.0    43    33   \n",
      "4  USC00200228  ANN ARBOR SE, MI US  2020-01-05  0.02   0.2   0.0    35    29   \n",
      "\n",
      "   TOBS  \n",
      "0    24  \n",
      "1    30  \n",
      "2    40  \n",
      "3    33  \n",
      "4    29  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        STATION                 NAME        DATE  PRCP  SNOW  SNWD  TMAX  \\\n",
      "85  USC00200228  ANN ARBOR SE, MI US  2020-03-26  0.00   0.0   0.0    57   \n",
      "86  USC00200228  ANN ARBOR SE, MI US  2020-03-27  0.27   0.0   0.0    58   \n",
      "87  USC00200228  ANN ARBOR SE, MI US  2020-03-28  1.54   0.0   0.0    51   \n",
      "88  USC00200228  ANN ARBOR SE, MI US  2020-03-29  0.64   0.0   0.0    50   \n",
      "89  USC00200228  ANN ARBOR SE, MI US  2020-03-30  0.03   0.0   0.0    62   \n",
      "\n",
      "    TMIN  TOBS  \n",
      "85    27    44  \n",
      "86    38    38  \n",
      "87    36    42  \n",
      "88    42    50  \n",
      "89    40    40  \n"
     ]
    }
   ],
   "source": [
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the width of your screen, the data may be printed in 1 or 2 \"blocks\". Ok, consider how awesome this is. With essentially one line of code, where we used `pd.read_csv()`, not only did we get all the data into a `DataFrame`, but that data is organized into rows and columns and the information about the columns names has already been added to the variable as well. In the \"USC00200228.csv\" file, all of the data is provided with quotes surrounding every entry which makes them look a bit like strings. Let's print the `dtypes` attribute of `data` to see what Pandas did with the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATION     object\n",
      "NAME        object\n",
      "DATE        object\n",
      "PRCP       float64\n",
      "SNOW       float64\n",
      "SNWD       float64\n",
      "TMAX         int64\n",
      "TMIN         int64\n",
      "TOBS         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we did the read, Pandas already figured out the appropriate data type of each column and stored the data appropriately. In other words, Pandas has handled all of the data entry with basically one line of code. We can now get a quick sense of the data using tools that you've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 90 rows of data as expected as my file contained data for 3 months of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 9)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `shape` attribute tells us, again 90 rows, and 9 columns. We can quickly get the metadata, if there is any:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STATION' 'NAME' 'DATE' 'PRCP' 'SNOW' 'SNWD' 'TMAX' 'TMIN' 'TOBS']\n",
      "RangeIndex(start=0, stop=90, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(data.columns.values)\n",
    "print(data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the column headers are present as expected. The row index values are generic: here just a range from 0 to 90 incrementing by 1. This is expected since we didn't tell Pandas how to index the data. `read_csv()` has an index keyword that we could have used like we did in the earlier examples.\n",
    "\n",
    "Finally, with the names of the columns, it is easy to grab data by column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2020-01-01\n",
      "1     2020-01-02\n",
      "2     2020-01-03\n",
      "3     2020-01-04\n",
      "4     2020-01-05\n",
      "         ...    \n",
      "85    2020-03-26\n",
      "86    2020-03-27\n",
      "87    2020-03-28\n",
      "88    2020-03-29\n",
      "89    2020-03-30\n",
      "Name: DATE, Length: 90, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   NAME  PRCP\n",
      "0   ANN ARBOR SE, MI US  0.05\n",
      "1   ANN ARBOR SE, MI US  0.00\n",
      "2   ANN ARBOR SE, MI US  0.00\n",
      "3   ANN ARBOR SE, MI US  0.00\n",
      "4   ANN ARBOR SE, MI US  0.02\n",
      "..                  ...   ...\n",
      "85  ANN ARBOR SE, MI US  0.00\n",
      "86  ANN ARBOR SE, MI US  0.27\n",
      "87  ANN ARBOR SE, MI US  1.54\n",
      "88  ANN ARBOR SE, MI US  0.64\n",
      "89  ANN ARBOR SE, MI US  0.03\n",
      "\n",
      "[90 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data[['NAME','PRCP']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of the double square brackets when printing multiple columns (e.g. I'm using a list of column names to \"index\" my dataframe.\n",
    "\n",
    "At this point, you should start to see some of the benefits of Pandas series' and data frames: they provide highly organized data structures that we can pull information from quickly and easily as well as tools to efficiently populate those data types. In the next lesson, we will further explore how we use these data types to do some quick \n",
    "analysis of our data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
